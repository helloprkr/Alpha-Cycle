# Concept

## Core Theory

Research on inference-time scaling and chain-of-thought reasoning in large language models. We explore how allocating more compute during inference (rather than training) can improve model performance on complex reasoning tasks.

## Key Definitions

- **Inference-time scaling**: Using additional computational resources during inference to improve output quality
- **Chain-of-thought (CoT)**: Prompting technique where models generate intermediate reasoning steps
- **Test-time compute**: Resources spent during inference rather than training

## Scope

Focus on 2024-2026 advances in:
1. Methods for scaling inference compute (e.g., MCTS, self-consistency, iterative refinement)
2. Reasoning model architectures (o1-style models)
3. Trade-offs between training compute and inference compute
