{
  "text": "The integration of recursion into language modeling represents a shift from static, feed-forward depth to dynamic, compute-optimal reasoning. This \"Technical Trace\" identifies how recursive structures\u2014whether in latent space, architecture, or prompt processing\u2014enable models to scale performance during inference by trading time for accuracy.\n\nFoundational and Emerging Recursive Architectures\n\nRecent research has moved beyond simple Chain-of-Thought (CoT) toward structural recursion, where the model iterates on internal representations or external outputs to refine its reasoning.\n\nRecursive Language Models (published 5 days ago): This work introduces RLMs as a general inference strategy that allows models to process arbitrarily long prompts and complex tasks through the lens of inference-time scaling. It reinterprets traditional context limits by treating prompt processing as a recursive task rather than a linear one. \nRecursive Language Models\nRecursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems (published a year ago): Researchers from Google DeepMind introduced Recursive INference Scaling (RINS), a \"plug-in\" recipe for scaling inference time. Inspired by the fractal geometry of language, RINS allows models to recursively apply inference compute to improve results in both language and multimodal domains. \nRecursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems\nScaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach (published a year ago): This paper explores a recurrent depth architecture that scales test-time computation by iterating a recurrent block in latent space. This \"unrolling\" of depth at inference time allows for implicit reasoning without the overhead of generating many intermediate tokens. \nScaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach\nScaling Laws and Dynamic Recursion\n\nA major theme in this domain is the transition from scaling parameters to scaling inference FLOPs. Recursive models are uniquely suited for this because their depth can be adjusted dynamically based on the complexity of the query.\n\nMixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation (published 6 months ago): This model learns to adjust its recursive depth dynamically per token. By allocating more \"recursive steps\" to difficult tokens and fewer to easy ones, it demonstrates an efficient path toward adaptive inference-time compute. \nMixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation\nScaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters (published a year ago): While not exclusively recursive, this foundational study from Google DeepMind and UC Berkeley established the scaling laws for test-time compute. It highlights that for many complex tasks, increasing inference compute (often through iterative or recursive methods) is more effective than further increasing the parameter count of the base model. \nScaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters\nEncode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts (published 3 months ago): This paper proposes a framework where models \"think\" via recursive latent updates between the encoding and decoding phases. It represents a \"middle ground\" between standard Transformers and full recurrent models, specifically optimized for test-time scaling. \nEncode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts\nRefinement and Recursive Reasoning Capabilities\n\nThe connection between recursion and \"System 2\" thinking is often mediated through alignment and self-correction loops.\n\nUnlocking Recursive Thinking of LLMs: Alignment via Refinement (published 7 months ago): This research focuses on how recursive thinking capabilities can be unlocked in models like the o1-series. It emphasizes \"Alignment via Refinement,\" where the model is trained to recursively improve its own reasoning paths. \nUnlocking Recursive Thinking of LLMs: Alignment via Refinement\nScaling Latent Reasoning via Looped Language Models (published 2 months ago): Looped Transformers are a specific form of recursive architecture where the same layers are reused iteratively. This paper argues that looped models under-leverage pre-training data and provides a framework to scale latent reasoning more effectively through these loops. \nScaling Latent Reasoning via Looped Language Models\nAccelerating Training of Recursive Reasoning Models with Curriculum Guided Adaptive Recursion (published 2 months ago): This work tackles the training efficiency of recursive models, proposing a curriculum that starts with shallow recursion and progressively increases depth, enabling \"tiny\" networks to match the reasoning performance of much larger traditional models. \nAccelerating Training of Recursive Reasoning Models with Curriculum Guided Adaptive Recursion",
  "papers": [
    {
      "title": "Recursive Language Models",
      "url": "https://alphaxiv.org/abs/2512.24601",
      "arxiv_id": "2512.24601"
    },
    {
      "title": "Recursive Inference Scaling: A Winning Path to Scalable Inference in Language and Multimodal Systems",
      "url": "https://alphaxiv.org/abs/2502.07503",
      "arxiv_id": "2502.07503"
    },
    {
      "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
      "url": "https://alphaxiv.org/abs/2502.05171",
      "arxiv_id": "2502.05171"
    },
    {
      "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation",
      "url": "https://alphaxiv.org/abs/2507.10524",
      "arxiv_id": "2507.10524"
    },
    {
      "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
      "url": "https://alphaxiv.org/abs/2408.03314",
      "arxiv_id": "2408.03314"
    },
    {
      "title": "Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts",
      "url": "https://alphaxiv.org/abs/2510.07358",
      "arxiv_id": "2510.07358"
    },
    {
      "title": "Unlocking Recursive Thinking of LLMs: Alignment via Refinement",
      "url": "https://alphaxiv.org/abs/2506.06009",
      "arxiv_id": "2506.06009"
    },
    {
      "title": "Scaling Latent Reasoning via Looped Language Models",
      "url": "https://alphaxiv.org/abs/2510.25741",
      "arxiv_id": "2510.25741"
    },
    {
      "title": "Accelerating Training of Recursive Reasoning Models with Curriculum Guided Adaptive Recursion",
      "url": "https://alphaxiv.org/abs/2511.08653",
      "arxiv_id": "2511.08653"
    }
  ],
  "timestamp": "2026-01-04T11:06:28.324975"
}