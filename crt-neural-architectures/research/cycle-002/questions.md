# Questions for Cycle 2

## Q1

Building on the foundational findings from Cycle 1 which identified RNS hardware acceleration and emergent modular structure in grokking, conduct a structured "Technical Trace" search on modular arithmetic in attention mechanisms and Mixture-of-Experts architectures.

**Technical Keywords** (use at least three parallel search queries spanning):
- Attention mechanisms: sparse attention, linear attention, modular attention, attention decomposition
- Mixture-of-Experts: expert routing, modular networks, conditional computation, expert specialization
- Number theory connections: prime factorization in neural networks, cyclic structures, group theory in deep learning
- Adjacent fields: modular neural networks, compositional learning, disentangled representations

**Bridging Logic Being Investigated:**
Evidence that (1) attention mechanisms exhibit or can exploit modular/cyclic structure, or (2) Mixture-of-Experts architectures have mathematical connections to CRT-style decomposition where experts act as parallel modular processing channels.

**Conceptual Narrative:**
We are investigating whether the routing in MoE architectures mirrors CRT decomposition (experts as coprime modular channels) and whether attention over large context can be decomposed into parallel attention over modular subspaces.

**Organization Filter:**
Prioritize contributions from Google Research (Switch Transformer, MoE work), DeepMind, Meta FAIR, and researchers working on efficient attention (Tri Dao, Shazeer).

Identify the 5-10 most significant papers from the last two years with high community engagement. For each paper, provide methodology summary, specific contribution to the bridging logic, publication timeframe, and exact link.

Highlight any papers that connect MoE or attention to number-theoretic or modular structures.

